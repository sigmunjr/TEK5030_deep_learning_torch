{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sigmunjr/TEK5030_deep_learning_torch/blob/main/TEK5030_deep_learning_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5kcpe6Ov08i"
   },
   "source": [
    "# TEK5030 - Deep learning Lab\n",
    "\n",
    "The goal of this exercise is to give you some hands on experiance with building and training *deep neural networks*.\n",
    "\n",
    "To simplify setup, this execise will be run in Jupyter notebook/colab. Still some fun exercises are hard to do in a web browser. I recommende those with a proper hardware setup to also try [EX3_live_training.py](https://gitlab.com/sigmund.rolfsjord/unik4690_deep_learning_lab/-/blob/master/EX3_live_training.py) from an older lab [TEK5030 Deep learning lab (2020)](https://gitlab.com/sigmund.rolfsjord/unik4690_deep_learning_lab). In this lab you can also find code for running detection networks live.\n",
    "\n",
    "For those interested in learning [Tensorflow](tensorflow.org), you can try exercises from [https://github.com/sigmunjr/TEK5030_deep_learning](https://github.com/sigmunjr/TEK5030_deep_learning).\n",
    "\n",
    "The exercise will be focused around the *deep learning framework* called [PyTorch](www.pytorch.org), as this is currently the most common framework. The differences compared too other frameworks e.g. are generally quite small. The primary goal of these frameworks is to facilitate running you code on accelerators such as **GPUs**, providing **automatic differentiation** of you code and provide **tools and structure** specific for deep learning.\n",
    "\n",
    "A large set of guides and tutorials can be found at [pytorch.org/tutorials](pytorch.org/tutorials). I highly recommend going through these tutorials if you plan on working further with deep learning.\n",
    "\n",
    "As the dataset is only available in the newest version of *torchvision* we need to reinstall torchvision and **restart** the kernel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLUrU5ptWXNn"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import torchvision\n",
    "except:\n",
    "    !pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83aT28PXcYf_"
   },
   "source": [
    "## Enabling and testing the GPU\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "Next, we'll confirm that we can connect to the GPU with torch:\n",
    "<a id='initialize_device'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HO9r8OX5H7FJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    device_nr = torch.cuda.current_device()\n",
    "    print(f'Found GPU device: {device_nr} of type: {torch.cuda.get_device_name(device)}')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('No GPU found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXnAqj3jcYgA"
   },
   "source": [
    "# Exercise 1: Basic PyTorch\n",
    "We will start by simply introducing a few relevant concepts in PyTorch. I will reuse variables from above cells, so if you get missing variables check those cells.\n",
    "\n",
    "## Tensors\n",
    "Tensors are the primary form for data in PyTorch. Data needs to be converted to tensors to use most of the PyTorch APIs. More on this can be found [Tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\n",
    "\n",
    "Tensors can easily be created directly from data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8tmaNsRcYgA"
   },
   "outputs": [],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSYimD5GcYgB"
   },
   "source": [
    "or from Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvmpf2NucYgC"
   },
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np *= 3\n",
    "#Convert back to numpy\n",
    "print(x_np.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06Bu3DT_cYgC"
   },
   "source": [
    "### Running on GPU\n",
    "To run computation on GPU, you need to explicitly send the data to the GPU, with *.to* or *.gpu()*.\n",
    "\n",
    "*This is using device set in [Enabling and testing the GPU](#Enabling-and-testing-the-GPU).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTd_GeRocYgD"
   },
   "outputs": [],
   "source": [
    "x_data_gpu = x_data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVK2pM5CcYgD"
   },
   "source": [
    "To get it back to det cpu you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pupz6MUpcYgE"
   },
   "outputs": [],
   "source": [
    "x_data_cpu = x_data.cpu()\n",
    "#Or\n",
    "x_data_cpu = x_data.to('cpu')\n",
    "print(f'x_data_gpu device: {x_data_gpu.device}, x_data_cpu device: {x_data_cpu.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2lQgx95cYgE"
   },
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "An important part of deep learning and deep learning frameworks is the automatic differentiation, you can read a more extensive guide [here](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html). You can use the gradients to jointly optimize the parameters of a deep learning model. The automatic differentiation can be enabled for tensors by setting *required_grad in the initiation or setting the property afterwards. If you use premade layers or functions like [torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), the required_grad will be set to **True** by default.\n",
    "\n",
    "Pytorch builds and keeps a computational graph with all intermediate results in memory until the .backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdpIO32WcYgE"
   },
   "outputs": [],
   "source": [
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3)\n",
    "\n",
    "b.requires_grad = True\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = ((z - y) ** 2).mean()\n",
    "print(f'loss: {loss} b gradients: {b.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lLYd9JQcYgF"
   },
   "source": [
    "The gradients for e.g. the loss will be computed by calling *backward* on the loss tensor. You can find the gradients with respect to e.g. *b*, in *b.grad*. If b.grad gets multiple gradients, they will all be added into b.grad. So for each run you will have to manually zero out the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glRKgKNzcYgF"
   },
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(f'loss: {loss} b gradients: {b.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDZ_VsfNcYgF"
   },
   "source": [
    "To minimize the loss you could then e.g. run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVSYhmZscYgG"
   },
   "outputs": [],
   "source": [
    "for i in range(20):  #run 20 steps of minimization\n",
    "    z = torch.matmul(x, w) + b\n",
    "    loss = ((z - y) ** 2).mean()\n",
    "    loss.backward()\n",
    "    b.data -= b.grad\n",
    "    print(f'loss: {loss}, b grad norm: {b.grad.norm()}')\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0aDLTX9zWXX"
   },
   "source": [
    "An output of a function where some variables **require_grad**, will also \"require_grad\", as they may need the gradients and data to calculate the gradients of the initial variable. In other worlds a trail of gradients will be generated downstream from an initial tensor.\n",
    "\n",
    "**OBS!** A typical error is to keep the variables that require_grad in memory to long e.g. storing them in a list.\n",
    "\n",
    "You can stop the gradients and therby delete the whole trail of gradients, by calling .detach() on a tensor.\n",
    "\n",
    "```python\n",
    "b.detach()\n",
    "```\n",
    "\n",
    "This will also stop the propagation of gradients from .backward() from that point, as you cannot propagate the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ1rGjW2cYgG"
   },
   "source": [
    "## Datasets and dataloaders\n",
    "PyTorch also have some built inn tools for working with datasets. I recommend reading through this tutorial for more information on the topic [Data tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "For these exercises i choose a dataset with different pets [Oxford-IIIT Pet](https://www.robots.ox.ac.uk/~vgg/data/pets/). This dataset is well suited because it is big enough to be interesting, but small enough to download in reasonable time. It also has both classification labels and segmentation masks.\n",
    "\n",
    "We will use the [dataset from torchvision](https://pytorch.org/vision/stable/datasets.html) to make things simple.\n",
    "\n",
    "### Loading and preprocessing the data\n",
    "Here we load the dataset and add a *transform* function that convert the images to float tensors between 0 and 1 and resize them to a more manageable size. We also transform the  You can read more about data transformations [here](https://pytorch.org/vision/stable/transforms.html#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gmLenPdcVRl"
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, PILToTensor, Compose, ConvertImageDtype, Resize, Lambda, InterpolationMode\n",
    "\n",
    "data_transforms = Compose([Resize([64, 64]), ToTensor()])\n",
    "label_transforms = Compose([Resize([64, 64], InterpolationMode.NEAREST), PILToTensor()])\n",
    "\n",
    "label_map = ['cat', 'dog']\n",
    "breads_to_cat_or_dog = [0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
    "                        0, 0, 1, 1, 1]\n",
    "\"\"\"\n",
    "Transforms segmentation image from PIL Image with values [0, 1, 2] (background, border, pet) to [0, 1] (background, pet) and\n",
    "Transforms labels from type of breed to cat vs dog.\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "def transform_labels(inp):\n",
    "    seg_img, label = inp\n",
    "    return torch.minimum(label_transforms(seg_img) - 1, torch.tensor(1)).to(torch.float32), breads_to_cat_or_dog[label]\n",
    "\n",
    "\n",
    "train_data = datasets.OxfordIIITPet('.', 'trainval', target_types=('segmentation', 'category'),\n",
    "                                    transform=data_transforms, download=True, target_transform=transform_labels)\n",
    "test_data = datasets.OxfordIIITPet('.', 'test', target_types=('segmentation', 'category'), transform=data_transforms,\n",
    "                                   download=True, target_transform=transform_labels)\n",
    "\n",
    "#Check first output\n",
    "for img, (seg_mask, label) in test_data:\n",
    "    print(f'Image with shape {img.shape} values in segmentation mask: {seg_mask.unique()} label: {label_map[label]}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrK2y3K8cYgH"
   },
   "source": [
    "### Prepare data for training with dataloader\n",
    "We use dataloaders to speed up fetching of data, and for convenience methods like batching and shuffling. Dataloader expect an object with implemented *\\_\\_len\\_\\_* and *\\_\\_get_item\\_\\_* methods.\n",
    "Using *num_workers* will create worker threads to read and process data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOPDKbHKcYgH"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjMApdfjcYgI"
   },
   "source": [
    "### Look through the data\n",
    "You should always spend some time inspecting your data. Just to make sure everything is as expected...\n",
    "\n",
    "***OBS! Torch normally processes images on the formate Channels x Height x Width, while plt.plot, OpenCV etc. uses Height x With x Channels. Therefore we need a permute to arrange the dimensions correctly before plotting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47xh6Zh6cYgI"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "number_of_rows = 5\n",
    "for i in range(number_of_rows):\n",
    "    img, (seg_mask, label) = test_data[np.random.randint(0, len(test_data))]\n",
    "    figure = plt.figure(figsize=(15, 15))\n",
    "    #Draw img\n",
    "    figure.add_subplot(1, 2, 1)\n",
    "    plt.title(label_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze().permute([1, 2, 0]))\n",
    "    #Draw mask\n",
    "    figure.add_subplot(1, 2, 2)\n",
    "    plt.title(\"mask\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(seg_mask.squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KOzKw9ncYgI"
   },
   "source": [
    "# Exercise 1: Simple Convolutional Neural network (CNN)\n",
    "\n",
    "In this execrise you are building a neural network to separate between cats and dogs. If you want, you can also try separating different breeds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1In7K8aElaJ"
   },
   "source": [
    "### Nearest neighbors in pixel space\n",
    "\n",
    "For fun we can check out which images are close in pixel space. This means we will find images with similar colored pixels in the same location. Often generally dark images will match with dark images and visa-versa. This makes it clear that we cannot separate cat and dogs with a shallow classification algorithm in pixel space directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R44wH4-uYB5-"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "def display(display_list, titles=None):\n",
    "    \"\"\" Plotting images in list \"\"\"\n",
    "    from matplotlib import cm\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        if titles != None:\n",
    "            plt.title(titles[i])\n",
    "        plt.imshow(\n",
    "            np.squeeze(display_list[i]).transpose((1, 2, 0)),\n",
    "            vmin=0,\n",
    "            vmax=1\n",
    "        )\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Pick the first 400 samples and take only the images of each sample\n",
    "nn_loader = DataLoader(train_data, batch_size=400, shuffle=True)\n",
    "\n",
    "# Extract the images to a numpy array of the 400 images of shape 200x3x128x128\n",
    "X = next(iter(nn_loader))[0].numpy()\n",
    "print(X.shape)\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=3)\n",
    "# Add images reshaped to vectors to nearest-neighbors tree\n",
    "nbrs.fit(X.reshape([X.shape[0], -1]))\n",
    "\n",
    "# Loop through 10 test images and display 3 nearest neighbors\n",
    "for i, (image, (mask, label)) in enumerate(test_data):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    distances, indices = nbrs.kneighbors(image.numpy().reshape((1, -1)))\n",
    "    show_img = display([\n",
    "        image.numpy(),\n",
    "        X[indices[0, 0]],\n",
    "        X[indices[0, 1]],\n",
    "        X[indices[0, 2]]\n",
    "    ], ['Image'] + list(distances[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t25zQpioTCOP"
   },
   "source": [
    "## Fully-convolutional network\n",
    "It's a good idea to implement the network as a fully-convolutional neural network. This means that the network will work for different input sizes.\n",
    "\n",
    "A fully-convolutional network use no fully-connected layers (matrix multiplications). They often consist of mostly convolutional layers, but also include other operations that does not require fixed input size like: add, concatenations, max (relu), batch normalization etc.\n",
    "\n",
    "## Torch Module\n",
    "I recommend using the Object-Oriented style of Torch Module, as it is the most flexible and are almost identical in both Keras/Tensorflow and PyTorch. In this approach you make subclass of [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=module#torch.nn.Module) and implement the **\\_\\_init\\_\\_** and **forward** method. In **\\_\\_init\\_\\_** you need to initialize everything that has state in your network, while **call** method actually run the network from output to input.\n",
    "\n",
    "You can use [torch.nn.Convd2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), for the convolutions.\n",
    "\n",
    "I also recommend using some pooling operations like [torch.nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) or [torch.nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html), or strides in the convolutions to get a larger field of view.\n",
    "\n",
    "You can check out this tutorial [Build Model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html), for pointer on how to implement a module.\n",
    "\n",
    "**TODO:**\n",
    "Fill out the *\\_\\_init\\_\\_* and *call* method with appropriate layers. Remember to use activation functions like [*ReLU*](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) between your convolutional layers, but do not apply an activation function after your final layer.\n",
    "\n",
    "Try to use only a few layers 4-6 convolutional layers, to save computation and avoid problems with vanishing gradients. With many layers you would need to either be very careful with initialization and learning rate or use normalization layers like BatchNormalization.\n",
    "\n",
    "The output of the *forward* function should have shape \\[*batch_size*, *num_classes*\\].\n",
    "\n",
    "**If the \"output_features\" parameter is *True* you should output the result before the last convolutional layer.**\n",
    "\n",
    "*[torch.mean](https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean) is used to average over the spatial dimension to get only one output vector for the whole image.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKAAblBdfp5b"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Conv2d, Linear, Dropout, ReLU, AvgPool2d, MaxPool2d\n",
    "from torch.nn.functional import relu, elu\n",
    "\n",
    "\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conv1 = Conv2d(3, 16, (3, 3))\n",
    "        # TODO: Initialize the layers of your network\n",
    "        # You can find different layers in torch.nn (https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
    "\n",
    "    def forward(self, x, output_features=False):\n",
    "        # TODO: Run the image through your network\n",
    "        x = self.conv1(x)\n",
    "        # Your input should be a [Batch_size x 3 x 32 x 32] sized tensor\n",
    "        # Your output should be a [Batch_size x num_classes] sized matrix\n",
    "        # Output features of your second last layer\n",
    "        if output_features: return x\n",
    "        # Return the result of your network\n",
    "        return torch.mean(x, dim=(2, 3))\n",
    "\n",
    "\n",
    "# Initializing the model\n",
    "NUM_CLASSES = 5\n",
    "BATCH_SIZE = 8\n",
    "model = SimpleNet(NUM_CLASSES)\n",
    "# Running the model with 8 random 128x128x3 images\n",
    "model_output = model(torch.from_numpy(np.random.random((BATCH_SIZE, 3, 128, 128)).astype(np.float32)))\n",
    "print('Model output:', model_output)\n",
    "print('Model output shape:', model_output.shape)\n",
    "\n",
    "assert model_output.shape == (BATCH_SIZE, NUM_CLASSES), \"Incorrect output size from call\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmUfiVCmfI8e"
   },
   "source": [
    "### Parameters\n",
    "If your using the predefined modules from *torch.nn*, the parameters of the layers are automatically added to the modules parameters. The parameters are just tensors, but the register in a hieraki, so nested modules easiely can access all parameters.\n",
    "\n",
    "If you want to use raw tensors as parameters for your model, you need to wrap your tensors in *torch.nn.parameter.Parameter*\n",
    "\n",
    "```python\n",
    "class MyModule(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyModule, self).__init__()\n",
    "    self.b = torch.nn.parameter.Parameter(torch.tensor(1))\n",
    "```\n",
    "\n",
    "You can iterate over parameters in a module as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi4J5Cohjlt2"
   },
   "outputs": [],
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPXhw3XYPNAS"
   },
   "source": [
    "### Training a Torch model\n",
    "As we saw in [Automatic Differentiation](#Automatic-Differentiation) we can get the gradients with respect to a tensor after a backward. The tensor can also be updated with that gradient. Still this can be quite a hassel with many parameters/tensors.\n",
    "\n",
    "PyTorch provide a simplified interface for this, with [torch.optim](https://pytorch.org/docs/stable/optim.html). An optimizer is initialized with the parameters or tensors it is supposed to optimize. With a torch.nn.Module this is simply done with e.g.:\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "```\n",
    "\n",
    "Calling *optimizer.step()* will then update the tensor based on the gradient information. Calling *optimizer.zero_grad()* will zero out all the gradients ahead of a new step. A training step could then be done as following:\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "ouput = model(x)\n",
    "loss = calculate_loss(x, target)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "In [torch.optim](https://pytorch.org/docs/stable/optim.html#algorithms) you can find many implementations of different optimization algorithms. They all subtrackt some scaled version of the gradients from the parameters, but use various information to decide the step-size/scale. For simple tests i prefere to use [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam) as it is more robust to the input parameters than many of the other methods (you will need less parameter tuneing).\n",
    "\n",
    "#### **Tips for training**\n",
    "Start by training your network with just a small dataset, e.g. one batch. \n",
    "\n",
    "To check you have properly initialized your weights and scaled your inputs you can check your initial loss. Your initial loss should be roughly equal to *-ln(1/N)*, where N is the number of classes.\n",
    "\n",
    "When training on a small version of your dataset your accuracy should steadily rise to 1. If it does not, you could try to adjust your learning rate. Your learning rate should generally be as high as possible, while still not increase your loss at the begining of your training.\n",
    "\n",
    "Adam is often a good optimizer for quick tests, as it requires comparably less *tuning* of the learning rate.\n",
    "\n",
    "You can quite easily calculate your own loss function, but for convinience you can use [torch.nn.functional.cross_entropy](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy).\n",
    "\n",
    "**TODO**:\n",
    "Train the network and validate that the loss goes down. The dataset are small so we cannot expect very good accuracy, additionally colab provide less resources than previously. Therefore **do not** expect a very good result, just better than random. While you could get a decent result with a small model, I recommend to move on after your model just get a loss sligtly below ln(num_classes)=0.693.\n",
    "\n",
    "Remember to create your dataloaders by running the cells in [Datasets and dataloader](#Datasets-and-dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4vM7LSpWk8Jy"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_13329/539308091.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mNUM_EPOCHS\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSimpleNet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_classes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m#TODO: Create loss function \"calculate_loss\" and optmizer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'SimpleNet' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 3\n",
    "model = SimpleNet(num_classes=2).to(device)\n",
    "\n",
    "#TODO: Create loss function \"calculate_loss\" and optmizer\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    tic = time.time()\n",
    "\n",
    "    for i, (img, (seg_mask, label)) in enumerate(train_dataloader):\n",
    "\n",
    "        # TODO: Run model, calculate loss and optimize parameters\n",
    "\n",
    "        running_loss += loss\n",
    "        if i % 10 == 9:  # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f, time: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10, time.time() - tic))\n",
    "            running_loss = 0.0\n",
    "            tic = time.time()\n",
    "    loss_accum = 0\n",
    "    acc_accum = 0\n",
    "    cnt = 0\n",
    "    for inputs_test, (seg_mask, labels_test) in test_dataloader:\n",
    "        inputs_test, labels_test = inputs_test.to(device), labels_test.to(device)\n",
    "        with torch.no_grad():\n",
    "            test_output = model(inputs_test)\n",
    "            test_loss = calculate_loss(test_output, labels_test)\n",
    "        test_acc = (test_output.argmax(1) == labels_test).double().mean()\n",
    "        loss_accum += test_loss.detach()\n",
    "        acc_accum += test_acc.detach()\n",
    "        cnt += 1\n",
    "    print(f'test acc: {acc_accum/cnt} test_loss: {loss_accum/cnt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1mRoOulECt3"
   },
   "source": [
    "### Visualizing nearest neighbor in feature space\n",
    "\n",
    "The neural network transforms the input so that cats and dogs can be separated. Generally ths will make cat be close to cats and dogs be close to dogs. We can often also see that close object share some characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ak2cQArgvPB1"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Pick the first 400 samples and take only the images of each sample\n",
    "nn_loader = DataLoader(train_data, batch_size=400, shuffle=True)\n",
    "\n",
    "# Extract the images to a numpy array of the 400 images of shape 200x3x128x128\n",
    "images_np = next(iter(nn_loader))[0].numpy()\n",
    "\n",
    "# Extract the images to a numpy array of the 200 images of shape 200x128x128x3\n",
    "X = model(torch.tensor(images_np).to(device), output_features=True).detach().cpu().numpy()\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=3)\n",
    "# Add images reshaped to vectors to nearest-neighbors tree\n",
    "print('XSHAPE', X.shape)\n",
    "nbrs.fit(X.reshape([X.shape[0], -1]))\n",
    "\n",
    "# Loop through 10 test images and display 3 nearest neighbors\n",
    "cnt = 0\n",
    "for image, (mask, label) in test_data:\n",
    "    if cnt == 10:\n",
    "        break\n",
    "    cnt += 1\n",
    "    distances, indices = nbrs.kneighbors(\n",
    "        model(image[None].to(device), output_features=True).detach().cpu().numpy().reshape((1, -1)))\n",
    "    show_img = display([\n",
    "        image.numpy(),\n",
    "        images_np[indices[0, 0]],\n",
    "        images_np[indices[0, 1]],\n",
    "        images_np[indices[0, 2]]\n",
    "    ], ['Image'] + list(distances[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SttjNGvMtdgL"
   },
   "source": [
    "### Visualizing [GRAD-CAM](http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf)\n",
    "\n",
    "There are many techniques for visualizing what is going on inside a neural network.\n",
    "\n",
    "The technique use the gradients of the output label with respect to the image in combination with the values of the activation map. With this information we can plot what regions of the input that are pointing toward a given label.\n",
    "\n",
    "We would e.g. expect that a cat would light up if we used the cat label for as input to Grad-CAM.\n",
    "\n",
    "With this technique we can check if the network actually use relvant information in the classification process and are not overfitted.\n",
    "\n",
    "Since the output is normalized, it will always highlight some regions. If the surroundings is highlighted it may simply mean that it found nothing \"doglike\" in the image.\n",
    "\n",
    "We use the visualization tool [Captum](https://captum.ai/) for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pyOt6U0tcNH"
   },
   "outputs": [],
   "source": [
    "#Install the visualization tool\n",
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQ4ck6CUtjkU"
   },
   "outputs": [],
   "source": [
    "from captum import attr\n",
    "from captum.attr import GuidedGradCam\n",
    "\n",
    "#Create gradcam for model\n",
    "#TODO: input model and models last layer necessarily not conv5\n",
    "gradcam = GuidedGradCam(model, model.conv5)\n",
    "\n",
    "cnt = 0\n",
    "label_map = ['CAT', 'DOG']\n",
    "\n",
    "for idx in np.random.choice(len(test_data), 10):\n",
    "    image, (mask, label) = test_data[idx]\n",
    "    if cnt == 10:\n",
    "        break\n",
    "    cnt += 1\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "    ax[0].imshow(image.permute(1, 2, 0).numpy())\n",
    "    ax[1].imshow(image.permute(1, 2, 0).numpy())\n",
    "    out = model(image[None].to(device)).softmax(1).detach()\n",
    "    image.requires_grad = True\n",
    "    attr_cat = gradcam.attribute(image[None].to(device), 0, interpolate_mode='bicubic').mean((0, 1)).detach().cpu()\n",
    "    attr_dog = gradcam.attribute(image[None].to(device), 1, interpolate_mode='bicubic').mean((0, 1)).detach().cpu()\n",
    "    print(attr_cat.shape)\n",
    "    attr_cat -= attr_cat.min()\n",
    "    attr_cat /= attr_cat.max() + 1e-8\n",
    "    attr_dog -= attr_dog.min()\n",
    "    attr_dog /= attr_dog.max() + 1e-8\n",
    "\n",
    "    ax[0].set_title('CAT')\n",
    "    ax[1].set_title('DOG')\n",
    "\n",
    "    ax[0].matshow(attr_cat, cmap='jet', alpha=0.8 * attr_cat)\n",
    "    ax[1].matshow(attr_dog, cmap='jet', alpha=0.8 * attr_dog)\n",
    "\n",
    "    plt.show()\n",
    "    print(f'{label_map[out[0].argmax()]}: {out.max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmiq-vx6PdZ9"
   },
   "source": [
    "### Finetuning a network\n",
    "You can quite easily download and run a pretrained network. Using a network that is trained on a similar, but larger dataset will often improve your result. Using a network pretrained on the ImageNet dataset will definetly work, as the dataset already includes different types of dogs and cats.\n",
    "\n",
    "You can e.g. load a pretrained network like this:\n",
    "\n",
    "    from torchvision import models\n",
    "    weights = models.ResNet18_Weights.DEFAULT\n",
    "    base_model = models.resnet18(weights=weights)\n",
    "\n",
    "We then want to swap out the top classification layer, as we want a different number of classes. So we have to make this layer for our self. You can check out the code for [resnet18\n",
    "here](https://github.com/pytorch/vision/blob/31a4ef9f815a86a924d0faa7709e091b5118f00d/torchvision/models/resnet.py#L266). By reading the code, you can se that the final layer can be swapped out by doing,\n",
    "\n",
    "    base_model.fc = ...\n",
    "\n",
    "To get maximal performance from the weights, we should also use the same preprocessing as used when training the pretrained weights. This can be done with:\n",
    "\n",
    "    transforms = weights.transforms()\n",
    "    img_preprocessed = transforms(images)\n",
    "\n",
    "Now the network can be used similar to a Conv2d layer. If you have a small dataset or few computational resourses, you can freeze the network parameters in the pretrained network like this:\n",
    "\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "Since the network is trained on ImageNet with a 1000 classes, your loaded network will have 1000 outputs. There are two common options to fix this.\n",
    "\n",
    "1. The simplest way is to override the networks fc layer e.g. like this:\n",
    "```\n",
    "num_ftrs  = self.base_model.fc.in_features\n",
    "net.fc = Linear(num_ftrs, num_classes)\n",
    "```\n",
    "\n",
    "2. Another more flexible, but more complicated method is to just use the pretrained modules submodules directly in your forward function.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "  x = self.net.conv1(x)\n",
    "  x = self.net.bn1(x)\n",
    "  ...\n",
    "```\n",
    "\n",
    "Here you have to look into how the base networks original forward function works.\n",
    "\n",
    "**TODO:**\n",
    "Use a pretrained network as a start of your model and create only the last layers. Train only the layers you have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tirX2oB_IZ_4"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "class FinetuneNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(FinetuneNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # TODO: Initialize the layers of your network\n",
    "\n",
    "    def forward(self, x, output_features=False):\n",
    "        # TODO: Run the image through your network\n",
    "        # Your input should be a [Batch_size x 3 x 32 x 32] sized tensor\n",
    "        # Your output should be a [Batch_size x num_classes] sized matrix\n",
    "        if output_features: return x\n",
    "        # Return the result of your network\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initializing the model\n",
    "NUM_CLASSES = 5\n",
    "BATCH_SIZE = 8\n",
    "model_fine = FinetuneNet(NUM_CLASSES).to(device)\n",
    "# Running the model with 8 random 128x128x3 images\n",
    "model_output = model_fine(torch.rand((BATCH_SIZE, 3, 128, 128)).to(device))\n",
    "print('Model output:', model_output)\n",
    "print('Model output shape:', model_output.shape)\n",
    "\n",
    "assert model_output.shape == (BATCH_SIZE, NUM_CLASSES), \"Incorrect output size from call\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_ekQr68aGNq"
   },
   "source": [
    "**TODO:**\n",
    "\n",
    "Train your new network.\n",
    "\n",
    "This should give a lot better performance, at least 0.9 in test accuracy!\n",
    "\n",
    "You can re-run the visualization code, with this model. This should give a better clustering of dog and cats, and even cluster different breeds.\n",
    "\n",
    "**TODO:** Initialize and finetune your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create loader with batch size that fits in memory\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=6)\n",
    "test_dataloader = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=1)\n",
    "num_classes = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternatively run this to test with the cifar dataset instead:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def add_dummy_segmentation():\n",
    "    def transform(labels):\n",
    "        return 0, labels\n",
    "\n",
    "    return transform\n",
    "\n",
    "\n",
    "data_transforms = Compose([ToTensor()])\n",
    "cifar_trainset = datasets.CIFAR10(root='.', train=True, download=True, transform=data_transforms,\n",
    "                                  target_transform=add_dummy_segmentation())\n",
    "cifar_testset = datasets.CIFAR10(root='.', train=False, download=True, transform=data_transforms,\n",
    "                                 target_transform=add_dummy_segmentation())\n",
    "num_classes = 10\n",
    "\n",
    "train_dataloader = DataLoader(cifar_testset, batch_size=32, shuffle=True, num_workers=6)\n",
    "test_dataloader = DataLoader(cifar_testset, batch_size=128, shuffle=False, num_workers=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDe4uGErMXvF"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "# TODO: Initialize and finetune your model\n",
    "\n",
    "\n",
    "#TODO: Create optimizer and loss function\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    tic = time.time()\n",
    "\n",
    "    for i, (img, (seg_mask, label)) in enumerate(train_dataloader):\n",
    "        # TODO: Run model, calculate loss and optimize parameters\n",
    "\n",
    "        running_loss += loss\n",
    "        if i % 10 == 9:  # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f, time: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10, time.time() - tic))\n",
    "            running_loss = 0.0\n",
    "            tic = time.time()\n",
    "    loss_accum = 0\n",
    "    acc_accum = 0\n",
    "    cnt = 0\n",
    "    for inputs_test, (seg_mask, labels_test) in test_dataloader:\n",
    "        inputs_test, labels_test = inputs_test.to(device), labels_test.to(device)\n",
    "        with torch.no_grad():\n",
    "            test_output = model_fine(inputs_test)\n",
    "            test_loss = calculate_loss(test_output, labels_test)\n",
    "        test_acc = (test_output.argmax(1) == labels_test).double().mean()\n",
    "        loss_accum += test_loss.detach()\n",
    "        acc_accum += test_acc.detach()\n",
    "        cnt += 1\n",
    "    print('TEST [%d, %5d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, i + 1, loss_accum / cnt, acc_accum / cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvD_ejXQXLfX"
   },
   "source": [
    "# Exercise 3: Segmentation\n",
    "We use the same dataset as in Exercise 1, but now instead of classifying dogs vs. cats, we try to output a segmentation mask. In other words simply classify each pixel, for this dataset simply into animal and not animal.\n",
    "\n",
    "First we load the dataset, just as last time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NCSIIPHXUrp"
   },
   "source": [
    "## Creating a segmentation network\n",
    "\n",
    "A segmentation network is very similar to a classification network, except that your output need to be in the same spatial dimentions as your input.\n",
    "\n",
    "### Simple segmentation network\n",
    "The simplest way of doing this is to run a fully-convolutional network (only convolutionas and â€¦Batch-size\\*C\\*N\\*M, then N and M should be larger than 1 (probably N, M > 5).\n",
    "\n",
    "You will probably get best results by using a pretrained network, but then you may need to use the second method of rewriting the *forward* function.\n",
    "\n",
    "I use [torch.nn.functional.interpolate](https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html) to resize the output of the network. This implements gradients, so we can run backpropagation through the function.\n",
    "\n",
    "## Loss function\n",
    "A typical approach is to let your network output a classification for each pixel. So instead of your network outputting a tensor of shape (N, K), you output a tensor of shape (N, K, H, W). Here N is the number of input images, K is the number of classes, while H and W are height and width. \n",
    "\n",
    "Each output along the K dimension should then represent the loglikelihood of a given class, for that pixel.\n",
    "\n",
    "Luckily both [torch.nn.functional.cross_entropy](https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy.html#torch.nn.functional.binary_cross_entropy)and [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) support arbitrary tensor size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMrOR_W0XfXI"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Conv2d, Linear, Dropout\n",
    "from torch.nn.functional import interpolate\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "\n",
    "class SimpleSegNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleSegNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # TODO: Initialize the layers of your network\n",
    "        # You can find different layers in torch.nn\n",
    "\n",
    "    def forward(self, x, output_features=False):\n",
    "        in_size = x.shape\n",
    "        # TODO: Run the image through your network\n",
    "        # Your input should be a [Batch_size x 3 x 32 x 32] sized tensor\n",
    "        # Your output should be a [Batch_size x num_classes] sized matrix\n",
    "\n",
    "        # Output features of your second last layer\n",
    "        if output_features: return x\n",
    "\n",
    "        # Return the result of your network\n",
    "        return interpolate(x, in_size[2:4], mode='bilinear')\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_loss(pred, label):\n",
    "        #TODO: Implement loss function\n",
    "        return torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEmCzYhxYyT9"
   },
   "source": [
    "## Train your network\n",
    "Try to train your network both with a premade loss and your own implementation of a loss function.\n",
    "\n",
    "I suggest something as simple ass L2 loss (mean squared error).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTNs-ZNfFebL"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "seg_model = SimpleSegNet(num_classes=2).to(device)\n",
    "model_fine = None\n",
    "model = None\n",
    "running_loss = 0.0\n",
    "\n",
    "#TODO: Create Optimizer\n",
    "\n",
    "calculate_loss = torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    tic = time.time()\n",
    "\n",
    "    for i, (img, (seg_mask, label)) in enumerate(train_dataloader):\n",
    "        # TODO: Run model, calculate loss and optimize parameters\n",
    "\n",
    "        running_loss += loss.detach()\n",
    "        if i % 10 == 9:  # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f, time: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10, time.time() - tic))\n",
    "            running_loss = 0.0\n",
    "            tic = time.time()\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    for inputs_test, (seg_mask, labels_test) in test_dataloader:\n",
    "        inputs_test, seg_mask = inputs_test.to(device), seg_mask.to(device)\n",
    "        with torch.no_grad():\n",
    "            test_output = seg_model(inputs_test)\n",
    "            test_losses.append(calculate_loss(test_output, seg_mask[:, 0].to(torch.int64)).cpu().numpy())\n",
    "            test_accs.append((test_output.argmax(1) == seg_mask).double().mean().cpu().numpy())\n",
    "    print('TEST [%d, %5d] loss: %.3f, acc: %.3f' %\n",
    "          (epoch + 1, i + 1, np.mean(test_losses), np.mean(test_accs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_K1pNzCXQk7-"
   },
   "source": [
    "## Visualizing your result\n",
    "\n",
    "**TODO**: Inside the loop iterating through the test data, run your model and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YiR-6J3Vf1k"
   },
   "outputs": [],
   "source": [
    "def display(display_list, titles=None, cmaps={}):\n",
    "    \"\"\" Plotting images in list \"\"\"\n",
    "    from matplotlib import cm\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        if titles != None:\n",
    "            plt.title(titles[i])\n",
    "        plt.imshow(\n",
    "            display_list[i],\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "            cmap=cmaps[i] if i in cmaps else None\n",
    "        )\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for image, (mask, label) in test_data:\n",
    "    if cnt == 4:\n",
    "        break\n",
    "    cnt += 1\n",
    "    # TODO: Get the output from your network\n",
    "    out = torch.nn.functional.softmax(seg_model(image[None].to(device)), 1).cpu().detach().numpy()[0, label, :, :]\n",
    "\n",
    "    #TODO: plot the output\n",
    "    display([image.permute(1, 2, 0).numpy(), mask.numpy().squeeze(), out.squeeze()], cmaps={2: 'jet'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pmk6D6Dnlf_T"
   },
   "source": [
    "## Extra challenge\n",
    "\n",
    "Using the streigh forward fully-convolutional approach is simple, but you do lose much of the spatial information, makeing the network less precise. A better approach is to us a [U-Net](https://arxiv.org/pdf/1505.04597.pdf) type architecture. Here you make skip-connection, either by simple addition or by concatinating the outputs. This can be done by only changing the *call* method in the network. Then the output can be resized multiple times with tf.image.resize and the skip-connection can be done with [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html) or simply +. It is not necessary to use max-pooling or up-convolution/Conv2DTransposed.\n",
    "\n",
    "![u-net](https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41598-019-53797-9/MediaObjects/41598_2019_53797_Fig1_HTML.png)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TEK5030_deep_learning_torch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
